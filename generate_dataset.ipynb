{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "import librosa as lib\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_file(file: str):\n",
    "    \n",
    "    \"\"\"\n",
    "    Import data from file.\n",
    "\n",
    "    Parameters:\n",
    "    --------------------------\n",
    "    file : str\n",
    "        The .wave file you want to import\n",
    "\n",
    "    Outputs:\n",
    "    --------------------------\n",
    "    sig : ndarray\n",
    "        Array containing signal amplitude value\n",
    "    sr : int\n",
    "        Value of the sampling rate\n",
    "    t : int\n",
    "        5*sr to generate signal of lenght 5s \n",
    "    \"\"\"\n",
    "\n",
    "    sig, sr = lib.load(file)\n",
    "    t = 5*sr\n",
    "    return sig, sr, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slices(sig, t: int):\n",
    "    \"\"\"\n",
    "    Make audio slices.\n",
    "\n",
    "    Parameters:\n",
    "    --------------------------\n",
    "    sig : ndarray\n",
    "        Array containing signal amplitude value\n",
    "    t : int\n",
    "        Multiple of the sampling rate to have a specific time length\n",
    "\n",
    "    Outputs:\n",
    "    --------------------------\n",
    "    li : list[ndarray]\n",
    "        List of different slices\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    li = []\n",
    "    for i in range(0,len(sig)-t,t):\n",
    "        li.append(np.asarray(sig[i:i+t]))\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitch_mod(data, sampling_rate:int, pitch_factor:int):\n",
    "\n",
    "    \"\"\"\n",
    "    Modulate the signal amplitude (for data augmentation purposes)\n",
    "\n",
    "    Parameters:\n",
    "    --------------------------\n",
    "    data: ndarray\n",
    "        Array containing signal amplitude value\n",
    "    sampling_rate : int\n",
    "        Sampling rate of the signal\n",
    "    pitch_factor : int\n",
    "        Number of semitones used for modulation\n",
    "    \n",
    "    Outputs:\n",
    "    --------------------------\n",
    "    pitch_shifted_data : ndarray\n",
    "        Array containing modulated signal amplitude value\n",
    "    \"\"\"\n",
    "\n",
    "    return lib.effects.pitch_shift(y=data, bins_per_octave=12, sr=sampling_rate, n_steps=pitch_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noising(data,noise_factor:float):\n",
    "\n",
    "    \"\"\"\n",
    "    Add noise to the signal (for data augmentation purposes)\n",
    "\n",
    "    Parameters:\n",
    "    --------------------------\n",
    "    data: ndarray\n",
    "        List containing signal amplitude value\n",
    "    noise_factor : float\n",
    "        Hyperparameter used to specify the importance of the noise\n",
    "    \n",
    "    Outputs:\n",
    "    --------------------------\n",
    "    noised_data : ndarray\n",
    "        Array containing noisy signal amplitude value\n",
    "    \"\"\"\n",
    "\n",
    "    noise = np.random.randn(len(data))\n",
    "    noisy_data = data + noise_factor * noise\n",
    "    # Cast back to same data type\n",
    "    noisy_data = noisy_data.astype(type(data[0]))\n",
    "    return np.asarray(noisy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_spectrogramm(li: list, sr: int):\n",
    "\n",
    "    \"\"\"\n",
    "    Generate mel-spectrograms from slices.\n",
    "\n",
    "    Parameters:\n",
    "    --------------------------\n",
    "    li: list[ndarray]\n",
    "        List containing slices\n",
    "    sr : int\n",
    "        Sampling rate\n",
    "    \n",
    "    Outputs:\n",
    "    --------------------------\n",
    "    spec : list[ndarray]\n",
    "        List of generated spectrograms\n",
    "    \"\"\"    \n",
    "\n",
    "    hl = 512 # number of samples per time-step in spectrogram\n",
    "    hi = 216 # Height of image\n",
    "    wi = 384 # Width of image\n",
    "\n",
    "    spec = []\n",
    "    for el in li:\n",
    "        S = lib.feature.melspectrogram(y=el, sr=sr, n_mels=hi, fmax=8000,hop_length=hl)\n",
    "        spec.append(S)\n",
    "\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(list_of_wavefile:list)->list:\n",
    "    \n",
    "    list_of_signals = []\n",
    "    for e in list_of_wavefile:\n",
    "        (s, sr, t) = load_from_file(e)\n",
    "        list_of_signals.extend(s)\n",
    "    \n",
    "    array_of_signals = np.asarray(list_of_signals)\n",
    "    list_of_slices = slices(array_of_signals,t)\n",
    "    list_of_spectrograms = gen_spectrogramm(list_of_slices,sr)\n",
    "\n",
    "    array_of_noisy_signals = noising(list_of_signals,0.1)\n",
    "    list_of_noisy_slices = slices(array_of_noisy_signals,t)\n",
    "    list_of_noisy_spectrograms = gen_spectrogramm(list_of_noisy_slices,sr)\n",
    "\n",
    "    array_of_pitched_signals = pitch_mod(array_of_signals,sr,3)\n",
    "    list_of_pitched_slices = slices(array_of_pitched_signals,t)\n",
    "    list_of_pitched_spectrograms = gen_spectrogramm(list_of_pitched_slices,sr)\n",
    "\n",
    "    spec = []\n",
    "    spec.extend(list_of_spectrograms)\n",
    "    spec.extend(list_of_noisy_spectrograms)\n",
    "    spec.extend(list_of_pitched_spectrograms)\n",
    "    rd.shuffle(spec)\n",
    "\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = preprocess_pipeline([\"Grego_chant.wav\",\"Capella_greg.wav\",\"Mass_grego.wav\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for e in spec:\n",
    "    output.append(np.reshape(e,(-1,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "buf_dict = {\"X\": output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (3/3 shards): 100%|██████████| 6393/6393 [00:01<00:00, 4381.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_dict(buf_dict)\n",
    "dataset = {\n",
    "    'train': train_dataset,\n",
    "}\n",
    "dataset = DatasetDict(dataset)\n",
    "path = 'data/hugging_face_dataset/'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "dataset.save_to_disk(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['X'],\n",
      "        num_rows: 6393\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'X': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = DatasetDict.load_from_disk(path)\n",
    "print(dataset_dict)\n",
    "dataset_dict['train'].features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HELP_MARCO-KnaxR7mK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
