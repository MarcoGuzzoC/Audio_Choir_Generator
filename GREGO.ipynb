{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/.local/share/virtualenvs/HELP_MARCO-KnaxR7mK/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from GREGOConfig import GREGOConfig\n",
    "from model import AutoEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerConfig:\n",
      "\n",
      "Data:\n",
      "+-----------------+\n",
      "| batch_size : 18 |\n",
      "+-----------------+\n",
      "\n",
      "Model:\n",
      "+--------------------+\n",
      "| n_embed    : 216   |\n",
      "| block_size : 216   |\n",
      "| dropout    : 0.2   |\n",
      "| bias       : 0     |\n",
      "+--------------------+\n",
      "\n",
      "Training Loop:\n",
      "+-------------------+\n",
      "| max_epochs : 1000 |\n",
      "+-------------------+\n",
      "\n",
      "AdamW Optimizer:\n",
      "+-----------------------+\n",
      "| learning_rate : 1e-05 |\n",
      "+-----------------------+\n",
      "\n",
      "System:\n",
      "+---------------+\n",
      "| device : cuda |\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = GREGOConfig()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetCustom(Dataset):\n",
    "    def __init__(self, dataset, device):\n",
    "        self.dataset = dataset\n",
    "        self.device = device\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.dataset[index]\n",
    "\n",
    "        X = torch.tensor(np.reshape(data[\"X\"], (config.block_size, config.n_embed)), dtype=torch.float32).to(self.device).unsqueeze(0)\n",
    "        \n",
    "        return {\n",
    "            \"X\":X\n",
    "                }\n",
    "    \n",
    "    def __len__(self) -> int :\n",
    "        return self.dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderFactory():\n",
    "    def __init__(self, path:str = 'data/hugging_face_dataset/', batch_size = 3, device = 'cpu'):\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        print(\"1. Loading dataset: ...\", end=\"\")        \n",
    "        self.dataset = load_from_disk(path, keep_in_memory=True)\n",
    "        print(\"\\r1. Loading dataset: done ✔️\")\n",
    "\n",
    "        print(\"2. Preprocess datasets: ...\", end=\"\")\n",
    "        train_validation_splits = self.dataset['train'].train_test_split(test_size=0.2)\n",
    "        validation_test_splits = train_validation_splits['test'].train_test_split(test_size=0.2)\n",
    "        print(\"\\r2. Preprocess datasets: done ✔️\")\n",
    "\n",
    "        print(\"3. Split datasets: ...\", end=\"\")\n",
    "        self.train_data = DatasetCustom(train_validation_splits['train'], self.device)\n",
    "        self.val_data = DatasetCustom(validation_test_splits['train'], self.device)\n",
    "        self.test_data = DatasetCustom(validation_test_splits['test'], self.device)\n",
    "        print(\"\\r3. Split datasets: done ✔️\")\n",
    "\n",
    "        self.dataloader_train = DataLoader(self.train_data, batch_size=batch_size, shuffle=True)\n",
    "        self.dataloader_val = DataLoader(self.val_data, batch_size=batch_size, shuffle=True)\n",
    "        self.dataloader_test = DataLoader(self.test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    def __len__(self) -> int :\n",
    "        print(\"\\033[95m\\033[1m\\033[4mNumber of data by datasets splits\\033[0m\")\n",
    "        print(f\"Train\\t\\t: {len(self.train_data)}\\t\\t-> {len(self.train_data)/self.batch_size}\")\n",
    "        print(f\"Validation\\t: {len(self.val_data)}\\t\\t-> {len(self.val_data)/self.batch_size}\")\n",
    "        print(f\"Test\\t\\t: {len(self.test_data)}\\t\\t-> {len(self.test_data)/self.batch_size}\")\n",
    "        total = len(self.train_data) + len(self.val_data) + len(self.test_data)\n",
    "        print(f\"Total\\t\\t: {total}\")\n",
    "        return total\n",
    "\n",
    "    def get_batch(self, split):\n",
    "        # choose the correct dataloader\n",
    "        if split == 'train':\n",
    "            dataloader = self.dataloader_train\n",
    "        elif split == 'val':\n",
    "            dataloader = self.dataloader_val\n",
    "        else:\n",
    "            dataloader = self.dataloader_test\n",
    "\n",
    "        for batch in dataloader:\n",
    "            # Move tensors to device\n",
    "            batch_on_device = {k: v for k, v in batch.items()}\n",
    "            yield batch_on_device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GREGO(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(GREGO, self).__init__()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 216, 216])\n"
     ]
    }
   ],
   "source": [
    "m = AutoEncoder(1)\n",
    "input = torch.randn(1, 1, 216, 216)\n",
    "output = m(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading dataset: done ✔️\n",
      "2. Preprocess datasets: done ✔️\n",
      "3. Split datasets: done ✔️\n",
      "\u001b[95m\u001b[1m\u001b[4mNumber of data by datasets splits\u001b[0m\n",
      "Train\t\t: 576\t\t-> 32.0\n",
      "Validation\t: 115\t\t-> 6.388888888888889\n",
      "Test\t\t: 29\t\t-> 1.6111111111111112\n",
      "Total\t\t: 720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DataLoaderFactory(device=config.device, batch_size=config.batch_size)\n",
    "batchs = dataset.get_batch('train')\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model, loss function and the optimizer\n",
    "model = AutoEncoder(1)\n",
    "model = model.to(torch.device(config.device))\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, dataset, epoch):\n",
    "    model = model.train()\n",
    "\n",
    "    total_train_loss = 0.0\n",
    "    batchs = dataset.get_batch('train')\n",
    "\n",
    "    num_train_batches = len(dataset.train_data) // config.batch_size\n",
    "    for i, batch in enumerate(batchs):\n",
    "        # Get the batch data\n",
    "        X = batch['X']\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output = model(X)\n",
    "        # Loss\n",
    "        loss = criterion(output, X)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss (you might want to print every N batches, not every batch)\n",
    "        print(f\"\\rEpoch [{epoch + 1}/{config.max_epochs}], Batch [{i}/{num_train_batches}], Loss: {loss.item()}\", end=\"\")\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / num_train_batches\n",
    "\n",
    "    return model, avg_train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(model, dataset, epoch):\n",
    "    print(f\"\\nValidating...\", end=\"\")\n",
    "    # Validation Evaluation\n",
    "    model = model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    val_batches = dataset.get_batch('val')\n",
    "\n",
    "    num_val_batches = len(dataset.val_data) // config.batch_size\n",
    "    with torch.no_grad():  # Disable gradient computations\n",
    "        for i, val_batch in enumerate(val_batches):\n",
    "            # Get the batch data\n",
    "            X = val_batch['X']\n",
    "            # Forward pass\n",
    "            val_outputs = model(X)\n",
    "            # Compute loss\n",
    "\n",
    "            val_loss = criterion(val_outputs, X)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / num_val_batches\n",
    "\n",
    "    print(f\"\\rValidation Loss after Epoch {epoch + 1}: {avg_val_loss}\\n\")\n",
    "\n",
    "\n",
    "    return avg_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Batch [31/32], Loss: 2277.494140625755\n",
      "Validation Loss after Epoch 1: 2581.271240234375\n",
      "\n",
      "Epoch [2/1000], Batch [31/32], Loss: 2245.024169921875\n",
      "Validation Loss after Epoch 2: 2576.9424641927085\n",
      "\n",
      "Epoch [3/1000], Batch [31/32], Loss: 2338.976806640625\n",
      "Validation Loss after Epoch 3: 2568.1048177083335\n",
      "\n",
      "Epoch [4/1000], Batch [31/32], Loss: 2189.651123046875\n",
      "Validation Loss after Epoch 4: 2535.6810302734375\n",
      "\n",
      "Epoch [5/1000], Batch [31/32], Loss: 2190.709472656255\n",
      "Validation Loss after Epoch 5: 2506.995157877604\n",
      "\n",
      "Epoch [6/1000], Batch [31/32], Loss: 2147.9562988281255\n",
      "Validation Loss after Epoch 6: 2415.1695963541665\n",
      "\n",
      "Epoch [7/1000], Batch [31/32], Loss: 1896.5252685546875\n",
      "Validation Loss after Epoch 7: 2267.7944946289062\n",
      "\n",
      "Epoch [8/1000], Batch [31/32], Loss: 1914.3535156259375\n",
      "Validation Loss after Epoch 8: 2099.3323364257812\n",
      "\n",
      "Epoch [9/1000], Batch [31/32], Loss: 1669.6243896484375\n",
      "Validation Loss after Epoch 9: 1894.6654866536458\n",
      "\n",
      "Epoch [10/1000], Batch [31/32], Loss: 1460.7518310546875\n",
      "Validation Loss after Epoch 10: 1721.0748087565105\n",
      "\n",
      "Epoch [11/1000], Batch [31/32], Loss: 1230.1519775390625\n",
      "Validation Loss after Epoch 11: 1519.4114176432292\n",
      "\n",
      "Epoch [12/1000], Batch [31/32], Loss: 1063.3796386718755\n",
      "Validation Loss after Epoch 12: 1348.536600748698\n",
      "\n",
      "Epoch [13/1000], Batch [31/32], Loss: 870.75866699218755\n",
      "Validation Loss after Epoch 13: 1209.9801025390625\n",
      "\n",
      "Epoch [14/1000], Batch [31/32], Loss: 844.5407714843752\n",
      "Validation Loss after Epoch 14: 1056.6746419270833\n",
      "\n",
      "Epoch [15/1000], Batch [31/32], Loss: 706.1496582031252\n",
      "Validation Loss after Epoch 15: 957.8333028157552\n",
      "\n",
      "Epoch [16/1000], Batch [31/32], Loss: 661.7961425781255\n",
      "Validation Loss after Epoch 16: 881.2320353190104\n",
      "\n",
      "Epoch [17/1000], Batch [31/32], Loss: 609.8939208984375\n",
      "Validation Loss after Epoch 17: 842.4358622233073\n",
      "\n",
      "Epoch [18/1000], Batch [31/32], Loss: 523.0460815429688\n",
      "Validation Loss after Epoch 18: 787.853515625\n",
      "\n",
      "Epoch [19/1000], Batch [31/32], Loss: 553.1226196289062\n",
      "Validation Loss after Epoch 19: 716.1700236002604\n",
      "\n",
      "Epoch [20/1000], Batch [31/32], Loss: 489.83840942382815\n",
      "Validation Loss after Epoch 20: 706.6563008626302\n",
      "\n",
      "Epoch [21/1000], Batch [31/32], Loss: 486.13922119140625\n",
      "Validation Loss after Epoch 21: 638.1813456217448\n",
      "\n",
      "Epoch [22/1000], Batch [31/32], Loss: 468.33679199218755\n",
      "Validation Loss after Epoch 22: 585.8460744222006\n",
      "\n",
      "Epoch [23/1000], Batch [31/32], Loss: 435.58978271484375\n",
      "Validation Loss after Epoch 23: 519.2956034342448\n",
      "\n",
      "Epoch [24/1000], Batch [31/32], Loss: 369.68685913085945\n",
      "Validation Loss after Epoch 24: 476.70069885253906\n",
      "\n",
      "Epoch [25/1000], Batch [31/32], Loss: 362.72525024414065\n",
      "Validation Loss after Epoch 25: 418.2465565999349\n",
      "\n",
      "Epoch [26/1000], Batch [31/32], Loss: 292.48843383789065\n",
      "Validation Loss after Epoch 26: 337.48974609375\n",
      "\n",
      "Epoch [27/1000], Batch [31/32], Loss: 243.90510559082035\n",
      "Validation Loss after Epoch 27: 275.1958516438802\n",
      "\n",
      "Epoch [28/1000], Batch [27/32], Loss: 268.23944091796875"
     ]
    }
   ],
   "source": [
    "# Early stopping parameters\n",
    "patience = 3  # Number of epochs with no improvement after which training will be stopped\n",
    "counter = 0  # To count number of epochs with no improvement\n",
    "best_val_loss = None  # To keep track of best validation loss\n",
    "best_model_state = None  # To store best model's state\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for epoch in range(config.max_epochs):\n",
    "    # training loop\n",
    "    model, epoch_train_loss = training_loop(model, dataset, epoch)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # validation loop\n",
    "    epoch_val_loss = validation_loop(model, dataset, epoch)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    # Check if this epoch's validation loss is the best we've seen so far.\n",
    "    if best_val_loss is None or epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        best_model_state = copy.deepcopy(model.state_dict())  # Save best model\n",
    "        counter = 0  # Reset counter\n",
    "    else:\n",
    "        counter += 1  # Increment counter as no improvement in validation loss\n",
    "\n",
    "    # If we've had patience number of epochs without improvement, stop training\n",
    "    if counter >= patience:\n",
    "        print(f\"Early stopping on epoch {epoch}. Best epoch was {epoch - counter} with val loss: {best_val_loss:.4f}.\")\n",
    "        break\n",
    "\n",
    "# Load best model weights\n",
    "model.load_state_dict(best_model_state)\n",
    "torch.save(model.state_dict(), 'out/best_model.pt')  # Save the best model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exemple_batches = dataset.get_batch('train')\n",
    "ex_batch = next(exemple_batches)\n",
    "X = ex_batch['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace the model\n",
    "traced_model = torch.jit.trace(model.to('cpu'), (X.to('cpu')))\n",
    "# Save the traced model\n",
    "traced_model.save(\"out/best_model_script.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HELP_MARCO-KnaxR7mK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
